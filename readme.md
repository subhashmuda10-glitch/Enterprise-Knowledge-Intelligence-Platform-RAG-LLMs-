# ğŸ“˜ Enterprise GenAI Knowledge Assistant (RAG-based Q&A System)

An enterprise-grade **Generative AI knowledge assistant** built using **Retrieval-Augmented Generation (RAG)** to enable accurate, explainable, and context-aware question answering over internal documents such as HR policies and manuals.

This system combines **semantic search** with **LLM-based generation**, ensuring answers are grounded in source documents and suitable for enterprise use cases.

---

## ğŸš€ Key Features

- ğŸ“„ Natural language Q&A over enterprise documents (PDFs)
- ğŸ” Semantic search using vector embeddings
- ğŸ§  Retrieval-Augmented Generation (RAG) pipeline
- ğŸ” Multi-query expansion for improved retrieval coverage
- ğŸ’¬ Conversational memory for multi-turn interactions
- ğŸ“š Source citations with document and page references
- âš¡ FastAPI backend with RESTful endpoints
- ğŸ” Designed with enterprise explainability and trust in mind

---

## ğŸ—ï¸ System Architecture (High-Level)

1. Documents are ingested and split into semantic chunks  
2. Chunks are converted into embeddings using Hugging Face models  
3. Embeddings are stored in ChromaDB (vector database)  
4. User queries are embedded and matched via similarity search  
5. Retrieved context is injected into an LLM prompt  
6. LLM generates grounded answers with source attribution  
7. FastAPI exposes the system as a production-ready API  

---

## ğŸ› ï¸ Tech Stack

- **Language:** Python  
- **LLM Framework:** LangChain  
- **Embeddings & LLMs:** Hugging Face Transformers, Sentence-Transformers, FLAN-T5  
- **Vector Database:** ChromaDB  
- **Backend API:** FastAPI, Uvicorn  
- **Search:** Semantic Vector Search, Multi-Query Expansion  
- **Memory:** In-memory conversational memory  
- **Environment:** Python virtual environment (venv)

---

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py              # FastAPI entry point
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ qa_chain.py          # Core RAG pipeline
â”‚   â”œâ”€â”€ memory.py            # Conversational memory
â”‚   â””â”€â”€ prompts.py           # Prompt templates
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ ingest_docs.py       # Document ingestion & chunking
â”œâ”€â”€ vectorstore/
â”‚   â””â”€â”€ chroma_store.py      # ChromaDB setup
â”œâ”€â”€ data/
â”‚   â””â”€â”€ HR_Policy_Manual.pdf # Sample documents
â”œâ”€â”€ venv/                    # Virtual environment
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


âš™ï¸ Setup Instructions

1ï¸âƒ£ Clone the Repository
bash
Copy code
git clone https://github.com/your-username/genai-knowledge-assistant.git
cd genai-knowledge-assistant

2ï¸âƒ£ Create & Activate Virtual Environment
bash
Copy code
python -m venv venv
Windows

bash
Copy code
venv\Scripts\activate
Linux / macOS

bash
Copy code
source venv/bin/activate

3ï¸âƒ£ Install Dependencies
bash
Copy code
pip install -r requirements.txt

ğŸ“¥ Document Ingestion
Place your PDF documents inside the data/ directory and run:

bash
Copy code
python ingestion/ingest_docs.py

This will:

Load documents

Split them into chunks

Generate embeddings

Store them in ChromaDB

â–¶ï¸ Running the Application
Run FastAPI Backend
bash
Copy code
uvicorn api.main:app --reload
API will be available at:

cpp
Copy code
http://127.0.0.1:8000
API Documentation (Swagger UI)
arduino
Copy code
http://127.0.0.1:8000/docs

ğŸ” API Endpoints
Health Check
bash
Copy code
GET /health
Response

json
Copy code
{
  "status": "ok"
}
Ask a Question
bash
Copy code
POST /ask
Request

json
Copy code
{
  "question": "What is the casual leave policy?"
}
Response

json
Copy code
{
  "answer": "...",
  "sources": [
    {
      "source": "HR_Policy_Manual.pdf",
      "page": 79
    }
  ]
}

ğŸ§  Conversational Memory
Stores recent userâ€“assistant interactions in memory (RAM)

Enables follow-up questions with contextual understanding

Implemented as a lightweight, session-based buffer

Can be extended to Redis or a database for production use

âš–ï¸ Design Trade-offs
Used open-source local models instead of paid APIs for cost and data privacy

In-memory conversational memory for simplicity

Vector-only search (hybrid search and reranking identified as future improvements)

CPU-based inference (GPU recommended for production)

ğŸ”® Future Enhancements
Hybrid search (BM25 + vector)

Reranking models for improved retrieval precision

Persistent memory (Redis / Database)

Authentication and role-based access

UI layer (Streamlit / React)

GPU-based inference

ğŸ“Œ Use Cases
HR policy assistants

Internal knowledge bases

Employee self-service portals

Enterprise document search

Compliance and audit support tools

ğŸ“„ License
This project is intended for learning, demonstration, and portfolio purposes.

ğŸ™Œ Acknowledgements
LangChain

Hugging Face

ChromaDB

FastAPI
