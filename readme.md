ğŸ“˜ Enterprise GenAI Knowledge Assistant (RAG-based Q&A System)

An enterprise-grade Generative AI knowledge assistant built using Retrieval-Augmented Generation (RAG) to enable accurate, explainable, and context-aware question answering over internal documents such as HR policies and manuals.

This system combines semantic search with LLM-based generation, ensuring answers are grounded in source documents and suitable for enterprise use cases.

ğŸš€ Key Features

ğŸ“„ Natural language Q&A over enterprise documents (PDFs)

ğŸ” Semantic search using vector embeddings

ğŸ§  Retrieval-Augmented Generation (RAG) pipeline

ğŸ” Multi-query expansion for improved retrieval coverage

ğŸ’¬ Conversational memory for multi-turn interactions

ğŸ“š Source citations with document and page references

âš¡ FastAPI backend with RESTful endpoints

ğŸ” Designed with enterprise explainability and trust in mind

ğŸ—ï¸ System Architecture (High-Level)

Documents are ingested and split into semantic chunks

Chunks are converted into embeddings using Hugging Face models

Embeddings are stored in ChromaDB (vector database)

User queries are embedded and matched via similarity search

Retrieved context is injected into an LLM prompt

LLM generates grounded answers with source attribution

FastAPI exposes the system as a production-ready API

ğŸ› ï¸ Tech Stack

Language: Python

LLM Framework: LangChain

Embeddings & LLMs: Hugging Face Transformers, Sentence-Transformers, FLAN-T5

Vector Database: ChromaDB

Backend API: FastAPI, Uvicorn

Search: Semantic Vector Search, Multi-Query Expansion

Memory: In-memory conversational memory

Environment: Python virtual environment (venv)

ğŸ“‚ Project Structure
.
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py              # FastAPI entry point
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ qa_chain.py          # Core RAG pipeline
â”‚   â”œâ”€â”€ memory.py            # Conversational memory
â”‚   â””â”€â”€ prompts.py           # Prompt templates
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ ingest_docs.py       # Document ingestion & chunking
â”œâ”€â”€ vectorstore/
â”‚   â””â”€â”€ chroma_store.py      # ChromaDB setup
â”œâ”€â”€ data/
â”‚   â””â”€â”€ HR_Policy_Manual.pdf # Sample documents
â”œâ”€â”€ venv/                    # Virtual environment
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/genai-knowledge-assistant.git
cd genai-knowledge-assistant

2ï¸âƒ£ Create & Activate Virtual Environment
python -m venv venv


Windows

venv\Scripts\activate


Linux / macOS

source venv/bin/activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

ğŸ“¥ Document Ingestion

Place your PDF documents inside the data/ directory and run:

python ingestion/ingest_docs.py


This will:

Load documents

Split them into chunks

Generate embeddings

Store them in ChromaDB

â–¶ï¸ Running the Application
Run FastAPI Backend
uvicorn api.main:app --reload


API will be available at:

http://127.0.0.1:8000

API Documentation (Swagger UI)
http://127.0.0.1:8000/docs

ğŸ” API Endpoints
Health Check
GET /health


Response

{ "status": "ok" }

Ask a Question
POST /ask


Request

{
  "question": "What is the casual leave policy?"
}


Response

{
  "answer": "...",
  "sources": [
    {
      "source": "HR_Policy_Manual.pdf",
      "page": 79
    }
  ]
}

ğŸ§  Conversational Memory

Stores recent userâ€“assistant interactions in memory (RAM)

Enables follow-up questions with contextual understanding

Implemented as a lightweight, session-based buffer

Designed for demos and can be extended to Redis or a database for production

âš–ï¸ Design Trade-offs

Used open-source local models instead of paid APIs for cost and data privacy

In-memory memory for simplicity (can be made persistent)

Vector-only search (hybrid search and reranking identified as future enhancements)

CPU-based inference (GPU recommended for production)

ğŸ”® Future Enhancements

Hybrid search (BM25 + vector)

Reranking models for improved retrieval precision

Persistent memory (Redis / DB)

Authentication and role-based access

UI layer (Streamlit / React)

GPU-based inference

ğŸ“Œ Use Cases

HR policy assistants

Internal knowledge bases

Employee self-service portals

Enterprise document search

Compliance and audit support tools

ğŸ“„ License

This project is for learning, demonstration, and portfolio purposes.

ğŸ™Œ Acknowledgements

LangChain

Hugging Face

ChromaDB

FastAPI

â­ Final Note

This project demonstrates end-to-end GenAI system design, combining retrieval, generation, explainability, and backend engineering â€” suitable for GenAI Engineer, AI Engineer, and ML Engineer roles.
